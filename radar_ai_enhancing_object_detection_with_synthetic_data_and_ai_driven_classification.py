# -*- coding: utf-8 -*-
"""Radar AI: Enhancing Object Detection with Synthetic Data and AI-driven Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18jhphh1cbrLwP5F-QsZbryExTb2_AHXX
"""

import numpy as np
import pandas as pd

# Generate a synthetic RADAR dataset
np.random.seed(42)  # For reproducibility
num_samples = 100  # Number of RADAR detections

# Generate random X, Y positions (meters)
x_positions = np.random.uniform(0, 50, num_samples)
y_positions = np.random.uniform(0, 50, num_samples)

# Generate random Doppler velocities (-5 to 5 m/s)
doppler_velocity = np.random.uniform(-5, 5, num_samples)

# Generate random reflectivity (0.5 to 1.0, simulating object reflectance)
reflectivity = np.random.uniform(0.5, 1.0, num_samples)

# Create a DataFrame
radar_data = pd.DataFrame({
    "timestamp": np.linspace(0, num_samples * 0.1, num_samples),  # Simulated timestamps
    "x": x_positions,
    "y": y_positions,
    "doppler_velocity": doppler_velocity,
    "reflectivity": reflectivity
})

# Save to CSV
file_name = "radar_sample.csv"
radar_data.to_csv(file_name, index=False)

# Display first few rows
print("First 5 rows of the dataset:")
print(radar_data.head())

# Provide a download link
from google.colab import files
files.download(file_name)



import matplotlib.pyplot as plt
import pandas as pd

# Load the generated RADAR dataset
radar_data = pd.read_csv("radar_sample.csv")

# Extract values
x = radar_data["x"]
y = radar_data["y"]
doppler = radar_data["doppler_velocity"]

# Plot RADAR detections
plt.figure(figsize=(8, 6))
plt.scatter(x, y, c=doppler, cmap="coolwarm", alpha=0.7)
plt.colorbar(label="Doppler Velocity (m/s)")
plt.xlabel("X Position (meters)")
plt.ylabel("Y Position (meters)")
plt.title("RADAR Object Detections (Raw Data)")
plt.show()

from pykalman import KalmanFilter

# Initialize Kalman Filter
kf = KalmanFilter(initial_state_mean=[0, 0], n_dim_obs=2)

# Apply filter to X, Y positions
filtered_state_means, _ = kf.filter(radar_data[["x", "y"]].values)

# Plot before & after filtering
plt.figure(figsize=(8, 6))
plt.scatter(radar_data["x"], radar_data["y"], label="Raw Data", alpha=0.3)
plt.scatter(filtered_state_means[:, 0], filtered_state_means[:, 1], label="Filtered Data", color="red")
plt.xlabel("X Position (meters)")
plt.ylabel("Y Position (meters)")
plt.title("RADAR Detections Before & After Kalman Filtering")
plt.legend()
plt.show()

pip install pykalman

from pykalman import KalmanFilter

# Initialize Kalman Filter
kf = KalmanFilter(initial_state_mean=[0, 0], n_dim_obs=2)

# Apply filter to X, Y positions
filtered_state_means, _ = kf.filter(radar_data[["x", "y"]].values)

# Plot before & after filtering
plt.figure(figsize=(8, 6))
plt.scatter(radar_data["x"], radar_data["y"], label="Raw Data", alpha=0.3)
plt.scatter(filtered_state_means[:, 0], filtered_state_means[:, 1], label="Filtered Data", color="red")
plt.xlabel("X Position (meters)")
plt.ylabel("Y Position (meters)")
plt.title("RADAR Detections Before & After Kalman Filtering")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Load filtered RADAR dataset
radar_data = pd.read_csv("radar_sample.csv")

# Extract features for clustering
X = radar_data[["doppler_velocity", "reflectivity"]].values

# Apply K-Means Clustering (Assuming 2 clusters: Moving vs. Stationary)
kmeans = KMeans(n_clusters=2, random_state=42)
radar_data["cluster"] = kmeans.fit_predict(X)

# Plot results
plt.figure(figsize=(8, 6))
plt.scatter(radar_data["x"], radar_data["y"], c=radar_data["cluster"], cmap="coolwarm", alpha=0.7)
plt.colorbar(label="Cluster (0 = Stationary, 1 = Moving)")
plt.xlabel("X Position (meters)")
plt.ylabel("Y Position (meters)")
plt.title("RADAR Object Clustering Based on Movement")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# Prepare features and labels
X = radar_data[["x", "y", "doppler_velocity", "reflectivity", "cluster"]]
y = (radar_data["cluster"] == 1).astype(int)  # Moving objects = 1, Stationary = 0

# Split into training & testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train an MLP (Multi-Layer Perceptron) Classifier
model = MLPClassifier(hidden_layer_sizes=(16, 8), max_iter=500, random_state=42)
model.fit(X_train_scaled, y_train)

# Evaluate the model
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)

print(f"Model Accuracy: {accuracy:.2f}")

train_accuracy = model.score(X_train_scaled, y_train)
test_accuracy = model.score(X_test_scaled, y_test)

print(f"Training Accuracy: {train_accuracy:.2f}")
print(f"Testing Accuracy: {test_accuracy:.2f}")

import numpy as np
import pandas as pd

# Load RADAR dataset
radar_data = pd.read_csv("radar_sample.csv")

# Define thresholds for object classification
def classify_object(doppler):
    if doppler > 2:  # Fast-moving objects (Cars)
        return "Car"
    elif -2 <= doppler <= 2:  # Slow-moving objects (Pedestrians)
        return "Pedestrian"
    else:  # Stationary objects
        return "Static Object"

# Apply classification
radar_data["object_class"] = radar_data["doppler_velocity"].apply(classify_object)

# Encode class labels (Car = 0, Pedestrian = 1, Static Object = 2)
label_mapping = {"Car": 0, "Pedestrian": 1, "Static Object": 2}
radar_data["class_label"] = radar_data["object_class"].map(label_mapping)

# Save modified dataset
radar_data.to_csv("radar_sample_with_classes.csv", index=False)

# Display the first few rows
print(radar_data.head())

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Prepare features and labels
X = radar_data[["x", "y", "doppler_velocity", "reflectivity"]]
y = radar_data["class_label"]

# Split data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate Model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f"Random Forest Model Accuracy: {accuracy:.2f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense
from tensorflow.keras.utils import to_categorical

# Convert labels to categorical (One-Hot Encoding)
y_cnn = to_categorical(y, num_classes=3)

# Reshape Data for CNN (Add Channel Dimension)
X_cnn = np.expand_dims(X.values, axis=2)

# Split into Train & Test
X_train, X_test, y_train, y_test = train_test_split(X_cnn, y_cnn, test_size=0.2, random_state=42)

# Build CNN Model
model = Sequential([
    Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)),
    Flatten(),
    Dense(64, activation='relu'),
    Dense(3, activation='softmax')  # 3 classes: Car, Pedestrian, Static Object
])

# Compile & Train Model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=10, batch_size=32)

# Evaluate Model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"CNN Model Accuracy: {test_acc:.2f}")

pip install seaborn

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Predict labels using the Random Forest model
# Get predicted probabilities
y_pred_probs = model.predict(X_test)

# Convert probabilities to class labels
y_pred = np.argmax(y_pred_probs, axis=1)

# Get true class labels from one-hot encoded y_test
y_test_labels = np.argmax(y_test, axis=1)

# Compute Confusion Matrix using class labels
conf_matrix = confusion_matrix(y_test_labels, y_pred)

# Plot Confusion Matrix
plt.figure(figsize=(6,5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Car", "Pedestrian", "Static Object"], yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Random Forest Model")
plt.show()

# Print Classification Report
print("Classification Report:\n", classification_report(y_test_labels, y_pred, target_names=["Car", "Pedestrian", "Static Object"]))

import numpy as np

# Convert Predictions to Class Labels
y_pred_cnn = model.predict(X_test)
y_pred_labels = np.argmax(y_pred_cnn, axis=1)
y_test_labels = np.argmax(y_test, axis=1)

# Compute Confusion Matrix
conf_matrix_cnn = confusion_matrix(y_test_labels, y_pred_labels)

# Plot Confusion Matrix
plt.figure(figsize=(6,5))
sns.heatmap(conf_matrix_cnn, annot=True, fmt="d", cmap="Reds", xticklabels=["Car", "Pedestrian", "Static Object"], yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - CNN Model")
plt.show()

# Print Classification Report
print("Classification Report:\n", classification_report(y_test_labels, y_pred_labels, target_names=["Car", "Pedestrian", "Static Object"]))



from sklearn.utils import resample

# Upsample the minority classes (Car & Static Objects)
car_data = radar_data[radar_data["class_label"] == 0]
pedestrian_data = radar_data[radar_data["class_label"] == 1]
static_object_data = radar_data[radar_data["class_label"] == 2]

car_data_upsampled = resample(car_data, replace=True, n_samples=len(pedestrian_data), random_state=42)
static_object_upsampled = resample(static_object_data, replace=True, n_samples=len(pedestrian_data), random_state=42)

# Combine all into a balanced dataset
balanced_data = pd.concat([car_data_upsampled, pedestrian_data, static_object_upsampled])
balanced_data.to_csv("balanced_radar_data.csv", index=False)

model.fit(X_train, y_train, epochs=30, batch_size=32)

radar_data["acceleration"] = radar_data["doppler_velocity"].diff().fillna(0)

radar_data["doppler_velocity"] += np.random.normal(0, 0.3, radar_data.shape[0])

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization

# Load real RADAR data (for training GAN)
radar_data = pd.read_csv("radar_sample_with_classes.csv")
real_samples = radar_data[["x", "y", "doppler_velocity", "reflectivity"]].values

# Normalize data between -1 and 1
real_samples = (real_samples - np.min(real_samples, axis=0)) / (np.max(real_samples, axis=0) - np.min(real_samples, axis=0))
real_samples = 2 * real_samples - 1  # Scale to range [-1, 1]

# Define GAN Parameters
latent_dim = 8  # Random noise vector size

# Generator Model
generator = Sequential([
    Dense(16, input_dim=latent_dim, activation=LeakyReLU(alpha=0.2)),
    BatchNormalization(),
    Dense(32, activation=LeakyReLU(alpha=0.2)),
    BatchNormalization(),
    Dense(4, activation='tanh')  # 4 output features (X, Y, Doppler Velocity, Reflectivity)
])

# Discriminator Model
discriminator = Sequential([
    Dense(32, input_dim=4, activation=LeakyReLU(alpha=0.2)),
    Dense(16, activation=LeakyReLU(alpha=0.2)),
    Dense(1, activation='sigmoid')  # Binary classification (Real vs. Fake)
])

# Compile Discriminator
discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])

# Combine Generator & Discriminator into GAN Model
discriminator.trainable = False  # Freeze discriminator while training generator
gan = Sequential([generator, discriminator])
gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))

# Train GAN
epochs = 5000
batch_size = 64

for epoch in range(epochs):
    # Generate Fake RADAR Data
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    generated_data = generator.predict(noise)

    # Select Real Samples
    real_batch = real_samples[np.random.randint(0, real_samples.shape[0], batch_size)]

    # Train Discriminator
    d_loss_real = discriminator.train_on_batch(real_batch, np.ones((batch_size, 1)))  # Real = 1
    d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))  # Fake = 0
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # Train Generator (via GAN model)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))  # Trick Discriminator

    # Print progress every 500 epochs
    if epoch % 500 == 0:
        print(f"Epoch {epoch}: Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}")

# Generate synthetic RADAR data after training
noise = np.random.normal(0, 1, (10, latent_dim))
synthetic_radar_data = generator.predict(noise)
print("Generated Synthetic RADAR Data:\n", synthetic_radar_data)

import os
print(os.listdir())  # List all files in the current working directory

import pandas as pd
import numpy as np

# Generate synthetic radar data
num_samples = 100
x_positions = np.random.uniform(0, 50, num_samples)
y_positions = np.random.uniform(0, 50, num_samples)
doppler_velocity = np.random.uniform(-5, 5, num_samples)
reflectivity = np.random.uniform(0.5, 1.0, num_samples)

# Define class based on Doppler velocity
def classify_object(doppler):
    if doppler > 2:
        return "Car"
    elif -2 <= doppler <= 2:
        return "Pedestrian"
    else:
        return "Static Object"

# Apply classification
labels = [classify_object(v) for v in doppler_velocity]
label_mapping = {"Car": 0, "Pedestrian": 1, "Static Object": 2}
class_labels = [label_mapping[label] for label in labels]

# Create DataFrame
radar_data = pd.DataFrame({
    "x": x_positions,
    "y": y_positions,
    "doppler_velocity": doppler_velocity,
    "reflectivity": reflectivity,
    "class_label": class_labels
})

# Save to CSV
file_name = "radar_sample_with_classes.csv"
radar_data.to_csv(file_name, index=False)
print(f"✅ File '{file_name}' has been created successfully!")

import os
print(os.listdir())  # Should now include 'radar_sample_with_classes.csv'

radar_data = pd.read_csv("radar_sample_with_classes.csv")  # Should now work

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization

# Load real RADAR data (for training GAN)
radar_data = pd.read_csv("radar_sample_with_classes.csv")
real_samples = radar_data[["x", "y", "doppler_velocity", "reflectivity"]].values

# Normalize data between -1 and 1
real_samples = (real_samples - np.min(real_samples, axis=0)) / (np.max(real_samples, axis=0) - np.min(real_samples, axis=0))
real_samples = 2 * real_samples - 1  # Scale to range [-1, 1]

# Define GAN Parameters
latent_dim = 8  # Random noise vector size

# Generator Model
generator = Sequential([
    Dense(16, input_dim=latent_dim, activation=LeakyReLU(alpha=0.2)),
    BatchNormalization(),
    Dense(32, activation=LeakyReLU(alpha=0.2)),
    BatchNormalization(),
    Dense(4, activation='tanh')  # 4 output features (X, Y, Doppler Velocity, Reflectivity)
])

# Discriminator Model
discriminator = Sequential([
    Dense(32, input_dim=4, activation=LeakyReLU(alpha=0.2)),
    Dense(16, activation=LeakyReLU(alpha=0.2)),
    Dense(1, activation='sigmoid')  # Binary classification (Real vs. Fake)
])

# Compile Discriminator
discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5), metrics=['accuracy'])

# Combine Generator & Discriminator into GAN Model
discriminator.trainable = False  # Freeze discriminator while training generator
gan = Sequential([generator, discriminator])
gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))

# Train GAN
epochs = 5000
batch_size = 64

for epoch in range(epochs):
    # Generate Fake RADAR Data
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    generated_data = generator.predict(noise)

    # Select Real Samples
    real_batch = real_samples[np.random.randint(0, real_samples.shape[0], batch_size)]

    # Train Discriminator
    d_loss_real = discriminator.train_on_batch(real_batch, np.ones((batch_size, 1)))  # Real = 1
    d_loss_fake = discriminator.train_on_batch(generated_data, np.zeros((batch_size, 1)))  # Fake = 0
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # Train Generator (via GAN model)
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))  # Trick Discriminator

    # Print progress every 500 epochs
    if epoch % 500 == 0:
        print(f"Epoch {epoch}: Discriminator Loss: {d_loss[0]}, Generator Loss: {g_loss}")

# Generate synthetic RADAR data after training
noise = np.random.normal(0, 1, (10, latent_dim))
synthetic_radar_data = generator.predict(noise)
print("Generated Synthetic RADAR Data:\n", synthetic_radar_data)

import matplotlib.pyplot as plt
import numpy as np

# Convert back to original scale (if normalized)
synthetic_radar_data = (synthetic_radar_data + 1) / 2  # Convert back to [0,1] range
synthetic_radar_data *= (np.max(real_samples, axis=0) - np.min(real_samples, axis=0))
synthetic_radar_data += np.min(real_samples, axis=0)

# Extract features
x_synthetic = synthetic_radar_data[:, 0]
y_synthetic = synthetic_radar_data[:, 1]
doppler_synthetic = synthetic_radar_data[:, 2]  # Color based on velocity

# Scatter plot of synthetic RADAR detections
plt.figure(figsize=(8, 6))
plt.scatter(x_synthetic, y_synthetic, c=doppler_synthetic, cmap="coolwarm", alpha=0.7)
plt.colorbar(label="Doppler Velocity (m/s)")
plt.xlabel("X Position (meters)")
plt.ylabel("Y Position (meters)")
plt.title("Synthetic RADAR Object Detections")
plt.grid(True)
plt.show()

# Extract real data features
x_real = real_samples[:, 0]
y_real = real_samples[:, 1]
doppler_real = real_samples[:, 2]  # Real Doppler velocity

plt.figure(figsize=(10, 6))

# Plot real data in blue
plt.scatter(x_real, y_real, c=doppler_real, cmap="coolwarm", alpha=0.6, label="Real Data")

# Plot synthetic data in red
plt.scatter(x_synthetic, y_synthetic, c=doppler_synthetic, cmap="coolwarm", alpha=0.6, marker="x", label="Synthetic Data")

plt.colorbar(label="Doppler Velocity (m/s)")
plt.xlabel("X Position (meters)")
plt.ylabel("Y Position (meters)")
plt.title("Real vs. Synthetic RADAR Data Comparison")
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd

# Convert synthetic data back to original scale
synthetic_radar_data = (synthetic_radar_data + 1) / 2  # Convert back to [0,1] range
synthetic_radar_data *= (np.max(real_samples, axis=0) - np.min(real_samples, axis=0))
synthetic_radar_data += np.min(real_samples, axis=0)

# Create a DataFrame for synthetic data
synthetic_df = pd.DataFrame(synthetic_radar_data, columns=["x", "y", "doppler_velocity", "reflectivity"])

# Assign random class labels (since GAN does not generate class labels)
synthetic_df["class_label"] = np.random.choice([0, 1, 2], size=len(synthetic_df))  # Randomly assign Car, Pedestrian, Static Object

# Load real data
real_df = pd.read_csv("radar_sample_with_classes.csv")

# Merge real and synthetic data
combined_data = pd.concat([real_df, synthetic_df], ignore_index=True)

# Shuffle dataset to mix real and synthetic samples
combined_data = combined_data.sample(frac=1).reset_index(drop=True)

# Save the new dataset
combined_data.to_csv("radar_combined_dataset.csv", index=False)
print("✅ Merged dataset saved as 'radar_combined_dataset.csv'")

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, classification_report

# Load merged dataset
data = pd.read_csv("radar_combined_dataset.csv")

# Split features and labels
X = data[["x", "y", "doppler_velocity", "reflectivity"]]
y = data["class_label"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train an MLP Classifier
model = MLPClassifier(hidden_layer_sizes=(16, 8), max_iter=500, random_state=42)
model.fit(X_train_scaled, y_train)

# Predict and evaluate
y_pred = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, y_pred)
print(f"✅ Model Accuracy: {accuracy:.2f}")
print(classification_report(y_test, y_pred))

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout

# Reshape input for CNN (since CNNs expect 3D input)
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Build a CNN Model
model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(3, activation='softmax')  # 3 classes: Car, Pedestrian, Static Object
])

# Compile & Train
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_data=(X_test_cnn, y_test))

# Evaluate
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"✅ CNN Model Accuracy: {test_acc:.2f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization

# Reshape input for CNN (since CNNs expect 3D input)
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Improved CNN Model
model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    BatchNormalization(),  # Normalize activations
    Conv1D(128, kernel_size=3, activation='relu'),
    Dropout(0.3),  # Regularization
    Conv1D(256, kernel_size=3, activation='relu'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # Avoid overfitting
    Dense(3, activation='softmax')  # 3 classes: Car, Pedestrian, Static Object
])

# Compile & Train
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train with more epochs & early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# Evaluate
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"✅ Improved CNN Model Accuracy: {test_acc:.2f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization

# Reshape input for CNN (since CNNs expect 3D input)
X_train_cnn = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Improved CNN Model
model = Sequential([
    Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1), padding='same'),  # Add padding='same'
    BatchNormalization(),  # Normalize activations
    Conv1D(128, kernel_size=3, activation='relu', padding='same'),  # Add padding='same'
    Dropout(0.3),  # Regularization
    Conv1D(256, kernel_size=3, activation='relu', padding='same'),  # Add padding='same'
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # Avoid overfitting
    Dense(3, activation='softmax')  # 3 classes: Car, Pedestrian, Static Object
])

# Compile & Train
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train with more epochs & early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# Evaluate
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"✅ Improved CNN Model Accuracy: {test_acc:.2f}")

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Predict on test set
y_pred_cnn = model.predict(X_test_cnn)
y_pred_cnn = tf.argmax(y_pred_cnn, axis=1).numpy()

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_cnn)

# Plot confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="coolwarm", xticklabels=["Car", "Pedestrian", "Static Object"], yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Improved CNN Model")
plt.show()

from sklearn.metrics import classification_report

# Get classification report
print(classification_report(y_test, y_pred_cnn, target_names=["Car", "Pedestrian", "Static Object"]))

from imblearn.over_sampling import SMOTE

# Apply SMOTE to balance dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

print("✅ Dataset balanced using SMOTE. New class distribution:")
print(pd.Series(y_train_balanced).value_counts())

import numpy as np

# Apply log-transform to reflectivity
X_train_balanced[:, 3] = np.log1p(X_train_balanced[:, 3])
X_test_scaled[:, 3] = np.log1p(X_test_scaled[:, 3])

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

# Reshape input for CNN
X_train_cnn = X_train_balanced.reshape((X_train_balanced.shape[0], X_train_balanced.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Fine-Tuned CNN Model
model = Sequential([
    Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    BatchNormalization(),
    Conv1D(128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01)),  # L2 regularization
    Dropout(0.3),
    Conv1D(256, kernel_size=5, activation='relu'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')  # 3 classes: Car, Pedestrian, Static Object
])

# Compile & Train with Early Stopping
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train_cnn, y_train_balanced, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# Evaluate
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"✅ Improved CNN Model Accuracy: {test_acc:.2f}")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.regularizers import l2

# Reshape input for CNN
X_train_cnn = X_train_balanced.reshape((X_train_balanced.shape[0], X_train_balanced.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))

# Fine-Tuned CNN Model
model = Sequential([
    Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train_cnn.shape[1], 1), padding='same'),  # Add padding='same' to the first Conv1D layer
    BatchNormalization(),
    Conv1D(128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01), padding='same'),  # Add padding='same' to the second Conv1D layer
    Dropout(0.3),
    Conv1D(256, kernel_size=5, activation='relu', padding='same'), # Add padding='same' to the third Conv1D layer
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')  # 3 classes: Car, Pedestrian, Static Object
])

# Compile & Train with Early Stopping
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

model.fit(X_train_cnn, y_train_balanced, epochs=50, batch_size=32, validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# Evaluate
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"✅ Improved CNN Model Accuracy: {test_acc:.2f}")

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Predict on test set
y_pred_cnn = model.predict(X_test_cnn)
y_pred_cnn = tf.argmax(y_pred_cnn, axis=1).numpy()

# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred_cnn)

# Plot confusion matrix
plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="coolwarm", xticklabels=["Car", "Pedestrian", "Static Object"], yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Final Optimized CNN Model")
plt.show()

import pandas as pd

# Assuming we have two datasets: real radar data and synthetic radar data
# Let's create example dataframes for merging

# Sample real radar data
real_radar_data = pd.DataFrame({
    "timestamp": [0.1, 0.2, 0.3, 0.4, 0.5],
    "x": [10, 20, 30, 40, 50],
    "y": [5, 15, 25, 35, 45],
    "doppler_velocity": [1.2, -3.4, 2.1, -1.5, 0.3],
    "reflectivity": [0.9, 0.8, 0.7, 0.85, 0.95],
    "label": ["Car", "Pedestrian", "Static Object", "Car", "Pedestrian"]
})

# Sample synthetic radar data
synthetic_radar_data = pd.DataFrame({
    "timestamp": [0.15, 0.25, 0.35, 0.45, 0.55],
    "x": [12, 22, 32, 42, 52],
    "y": [7, 17, 27, 37, 47],
    "doppler_velocity": [1.0, -3.2, 2.3, -1.2, 0.5],
    "reflectivity": [0.88, 0.82, 0.72, 0.87, 0.92],
    "label": ["Car", "Pedestrian", "Static Object", "Car", "Pedestrian"]
})

# Merge real and synthetic datasets
merged_radar_data = pd.concat([real_radar_data, synthetic_radar_data], ignore_index=True)

# Save to CSV
csv_filename = "/mnt/data/merged_radar_data.csv"
merged_radar_data.to_csv(csv_filename, index=False)

# Display the merged dataset
import ace_tools as tools
tools.display_dataframe_to_user(name="Merged RADAR Dataset", dataframe=merged_radar_data)

# Provide download link
csv_filename

import os

# Create the directory if it doesn't exist
if not os.path.exists("/mnt/data"):
    os.makedirs("/mnt/data")

# Now save the CSV file
csv_filename = "/mnt/data/merged_radar_data.csv"
merged_radar_data.to_csv(csv_filename, index=False)

import pandas as pd

# Sample real radar data
real_radar_data = pd.DataFrame({
    "timestamp": [0.1, 0.2, 0.3, 0.4, 0.5],
    "x": [10, 20, 30, 40, 50],
    "y": [5, 15, 25, 35, 45],
    "doppler_velocity": [1.2, -3.4, 2.1, -1.5, 0.3],
    "reflectivity": [0.9, 0.8, 0.7, 0.85, 0.95],
    "label": ["Car", "Pedestrian", "Static Object", "Car", "Pedestrian"]
})

# Sample synthetic radar data
synthetic_radar_data = pd.DataFrame({
    "timestamp": [0.15, 0.25, 0.35, 0.45, 0.55],
    "x": [12, 22, 32, 42, 52],
    "y": [7, 17, 27, 37, 47],
    "doppler_velocity": [1.0, -3.2, 2.3, -1.2, 0.5],
    "reflectivity": [0.88, 0.82, 0.72, 0.87, 0.92],
    "label": ["Car", "Pedestrian", "Static Object", "Car", "Pedestrian"]
})

# Merge real and synthetic datasets
merged_radar_data = pd.concat([real_radar_data, synthetic_radar_data], ignore_index=True)

# Save to CSV
merged_radar_data.to_csv("merged_radar_data.csv", index=False)

print("✅ Merged dataset saved as 'merged_radar_data.csv' in your working directory.")

from google.colab import files
files.download("merged_radar_data.csv")

import numpy as np
import tensorflow as tf
import time

# Load the trained CNN model
model = tf.keras.models.load_model("trained_radar_cnn_model.h5")  # Ensure your trained model is saved

# AI Agent Class
class RadarAI_Agent:
    def __init__(self, model):
        self.model = model

    def process_radar_data(self, radar_input):
        """
        Process a new incoming RADAR data point.
        :param radar_input: A single RADAR sample [x, y, doppler_velocity, reflectivity]
        """
        radar_input = np.array(radar_input).reshape(1, 4, 1)  # Reshape for CNN model

        # Make a prediction
        prediction = self.model.predict(radar_input)
        predicted_class = np.argmax(prediction)

        # Interpret prediction
        class_labels = {0: "Car", 1: "Pedestrian", 2: "Static Object"}
        detected_object = class_labels[predicted_class]

        # Take action based on detected object
        self.take_action(detected_object)

        return detected_object

    def take_action(self, detected_object):
        """
        Define what action the agent takes based on detected object.
        """
        if detected_object == "Car":
            print("🚗 Detected: Car | Action: Continue monitoring speed & position.")
        elif detected_object == "Pedestrian":
            print("🚶 Detected: Pedestrian | Action: Trigger safety alert!")
        elif detected_object == "Static Object":
            print("🛑 Detected: Static Object | Action: Consider alternative route.")

    def run_agent(self, radar_stream):
        """
        Simulate real-time RADAR data input to the agent.
        :param radar_stream: List of RADAR samples
        """
        for radar_sample in radar_stream:
            print("\nProcessing RADAR data:", radar_sample)
            detected_object = self.process_radar_data(radar_sample)
            time.sleep(1)  # Simulating real-time delay

ls -l

!pip install pykalman
!pip install seaborn
!pip install imbalanced-learn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from pykalman import KalmanFilter
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
import seaborn as sns
from imblearn.over_sampling import SMOTE
from tensorflow.keras.regularizers import l2
import time

# Generate a synthetic RADAR dataset
np.random.seed(42)  # For reproducibility
num_samples = 100  # Number of RADAR detections

# Generate random X, Y positions (meters)
x_positions = np.random.uniform(0, 50, num_samples)
y_positions = np.random.uniform(0, 50, num_samples)

# Generate random Doppler velocities (-5 to 5 m/s)
doppler_velocity = np.random.uniform(-5, 5, num_samples)

# Generate random reflectivity (0.5 to 1.0, simulating object reflectance)
reflectivity = np.random.uniform(0.5, 1.0, num_samples)

# Create a DataFrame
radar_data = pd.DataFrame({
    "timestamp": np.linspace(0, num_samples * 0.1, num_samples),  # Simulated timestamps
    "x": x_positions,
    "y": y_positions,
    "doppler_velocity": doppler_velocity,
    "reflectivity": reflectivity
})

# Save to CSV
file_name = "radar_sample.csv"
radar_data.to_csv(file_name, index=False)

# Display first few rows
print("First 5 rows of the dataset:")
print(radar_data.head())

# --- Data Preprocessing ---
# Load the generated RADAR dataset
radar_data = pd.read_csv("radar_sample.csv")

# Extract features for clustering
X = radar_data[["doppler_velocity", "reflectivity"]].values

# Apply K-Means Clustering (Assuming 2 clusters: Moving vs. Stationary)
kmeans = KMeans(n_clusters=2, random_state=42)
radar_data["cluster"] = kmeans.fit_predict(X)

# Define thresholds for object classification
def classify_object(doppler):
    if doppler > 2:  # Fast-moving objects (Cars)
        return "Car"
    elif -2 <= doppler <= 2:  # Slow-moving objects (Pedestrians)
        return "Pedestrian"
    else:  # Stationary objects
        return "Static Object"

# Apply classification
radar_data["object_class"] = radar_data["doppler_velocity"].apply(classify_object)

# Encode class labels (Car = 0, Pedestrian = 1, Static Object = 2)
label_mapping = {"Car": 0, "Pedestrian": 1, "Static Object": 2}
radar_data["class_label"] = radar_data["object_class"].map(label_mapping)

# Save modified dataset
radar_data.to_csv("radar_sample_with_classes.csv", index=False)

# Prepare features and labels
X = radar_data[["x", "y", "doppler_velocity", "reflectivity", "cluster"]]
y = (radar_data["cluster"] == 1).astype(int)  # Moving objects = 1, Stationary = 0

# Split into training & testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE to balance dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

# Apply log-transform to reflectivity
X_train_balanced[:, 3] = np.log1p(X_train_balanced[:, 3])
X_test_scaled[:, 3] = np.log1p(X_test_scaled[:, 3])

# Reshape input for CNN
X_train_cnn = X_train_balanced.reshape((X_train_balanced.shape[0], X_train_balanced.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))
# --- End of Data Preprocessing ---

# --- Train and Save CNN Model ---
# 1. Define Model Architecture
model = Sequential([
    Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train_cnn.shape[1], 1), padding='same'),
    BatchNormalization(),
    Conv1D(128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01), padding='same'),
    Dropout(0.3),
    Conv1D(256, kernel_size=5, activation='relu', padding='same'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

# 2. Compile the Model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 3. Train the Model
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model.fit(X_train_cnn, y_train_balanced, epochs=50, batch_size=32,
          validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# 4. Save the Model
model.save("trained_radar_cnn_model.h5")
print("✅ Model trained and saved as 'trained_radar_cnn_model.h5'")
# --- End of Training and Saving ---


# AI Agent Class
class RadarAI_Agent:
    def __init__(self, model):
        self.model = model

    def process_radar_data(self, radar_input):
        """
        Process a new incoming RADAR data point.
        :param radar_input: A single RADAR sample [x, y, doppler_velocity, reflectivity]
        """
        radar_input = np.array(radar_input).reshape(1, 4, 1)  # Reshape for CNN model

        # Make a prediction
        prediction = self.model.predict(radar_input)
        predicted_class = np.argmax(prediction)

        # Interpret prediction
        class_labels = {0: "Car", 1: "Pedestrian", 2: "Static Object"}
        detected_object = class_labels[predicted_class]

        # Take action based on detected object
        self.take_action(detected_object)

        return detected_object

    def take_action(self, detected_object):
        """
        Define what action the agent takes based on detected object.
        """
        if detected_object == "Car":
            print("🚗 Detected: Car | Action: Continue monitoring speed & position.")
        elif detected_object == "Pedestrian":
            print("🚶 Detected: Pedestrian | Action: Trigger safety alert!")
        elif detected_object == "Static Object":
            print("🛑 Detected: Static Object | Action: Consider alternative route.")

    def run_agent(self, radar_stream):
        """
        Simulate real-time RADAR data input to the agent.
        :param radar_stream: List of RADAR samples
        """
        for radar_sample in radar_stream:
            print("\nProcessing RADAR data:", radar_sample)
            detected_object = self.process_radar_data(radar_sample)
            time.sleep(1)  # Simulating real-time delay

# Load the trained CNN model
model = tf.keras.models.load_model("trained_radar_cnn_model.h5")

# Create and run the AI agent
agent = RadarAI_Agent(model)
# Sample radar stream data
radar_stream = [[10, 5, 1.2, 0.9], [20, 15, -3.4, 0.8], [30, 25, 2.1, 0.7]]
agent.run_agent(radar_stream)

!pip install pykalman
!pip install seaborn
!pip install imbalanced-learn

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
from pykalman import KalmanFilter
from sklearn.ensemble import RandomForestClassifier
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.utils import to_categorical
import seaborn as sns
from imblearn.over_sampling import SMOTE
from tensorflow.keras.regularizers import l2
import time

# Generate a synthetic RADAR dataset
np.random.seed(42)  # For reproducibility
num_samples = 100  # Number of RADAR detections

# Generate random X, Y positions (meters)
x_positions = np.random.uniform(0, 50, num_samples)
y_positions = np.random.uniform(0, 50, num_samples)

# Generate random Doppler velocities (-5 to 5 m/s)
doppler_velocity = np.random.uniform(-5, 5, num_samples)

# Generate random reflectivity (0.5 to 1.0, simulating object reflectance)
reflectivity = np.random.uniform(0.5, 1.0, num_samples)

# Create a DataFrame
radar_data = pd.DataFrame({
    "timestamp": np.linspace(0, num_samples * 0.1, num_samples),  # Simulated timestamps
    "x": x_positions,
    "y": y_positions,
    "doppler_velocity": doppler_velocity,
    "reflectivity": reflectivity
})

# Save to CSV
file_name = "radar_sample.csv"
radar_data.to_csv(file_name, index=False)

# Display first few rows
print("First 5 rows of the dataset:")
print(radar_data.head())

# --- Data Preprocessing ---
# Load the generated RADAR dataset
radar_data = pd.read_csv("radar_sample.csv")

# Extract features for clustering
X = radar_data[["doppler_velocity", "reflectivity"]].values

# Apply K-Means Clustering (Assuming 2 clusters: Moving vs. Stationary)
kmeans = KMeans(n_clusters=2, random_state=42)
radar_data["cluster"] = kmeans.fit_predict(X)

# Define thresholds for object classification
def classify_object(doppler):
    if doppler > 2:  # Fast-moving objects (Cars)
        return "Car"
    elif -2 <= doppler <= 2:  # Slow-moving objects (Pedestrians)
        return "Pedestrian"
    else:  # Stationary objects
        return "Static Object"

# Apply classification
radar_data["object_class"] = radar_data["doppler_velocity"].apply(classify_object)

# Encode class labels (Car = 0, Pedestrian = 1, Static Object = 2)
label_mapping = {"Car": 0, "Pedestrian": 1, "Static Object": 2}
radar_data["class_label"] = radar_data["object_class"].map(label_mapping)

# Save modified dataset
radar_data.to_csv("radar_sample_with_classes.csv", index=False)

# Prepare features and labels
X = radar_data[["x", "y", "doppler_velocity", "reflectivity", "cluster"]]
y = (radar_data["cluster"] == 1).astype(int)  # Moving objects = 1, Stationary = 0

# Split into training & testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE to balance dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

# Apply log-transform to reflectivity
X_train_balanced[:, 3] = np.log1p(X_train_balanced[:, 3])
X_test_scaled[:, 3] = np.log1p(X_test_scaled[:, 3])

# Reshape input for CNN
X_train_cnn = X_train_balanced.reshape((X_train_balanced.shape[0], X_train_balanced.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))
# --- End of Data Preprocessing ---

# --- Train and Save CNN Model ---
# 1. Define Model Architecture
model = Sequential([
    Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train_cnn.shape[1], 1), padding='same'),
    BatchNormalization(),
    Conv1D(128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01), padding='same'),
    Dropout(0.3),
    Conv1D(256, kernel_size=5, activation='relu', padding='same'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

# 2. Compile the Model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 3. Train the Model
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model.fit(X_train_cnn, y_train_balanced, epochs=50, batch_size=32,
          validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# 4. Save the Model
model.save("trained_radar_cnn_model.h5")
print("✅ Model trained and saved as 'trained_radar_cnn_model.h5'")
# --- End of Training and Saving ---


# AI Agent Class
class RadarAI_Agent:
    def __init__(self, model):
        self.model = model

    def process_radar_data(self, radar_input):
        """
        Process a new incoming RADAR data point.
        :param radar_input: A single RADAR sample [x, y, doppler_velocity, reflectivity]
        """
        # Add a dummy feature to match the expected input shape (5 features)
        radar_input = np.append(radar_input, 0)
        radar_input = np.array(radar_input).reshape(1, 5, 1)  # Reshape for CNN model (5 features, 1 channel)

        # Make a prediction
        prediction = self.model.predict(radar_input)
        predicted_class = np.argmax(prediction)

        # Interpret prediction
        class_labels = {0: "Car", 1: "Pedestrian", 2: "Static Object"}
        detected_object = class_labels[predicted_class]

        # Take action based on detected object
        self.take_action(detected_object)

        return detected_object

    def take_action(self, detected_object):
        """
        Define what action the agent takes based on detected object.
        """
        if detected_object == "Car":
            print("🚗 Detected: Car | Action: Continue monitoring speed & position.")
        elif detected_object == "Pedestrian":
            print("🚶 Detected: Pedestrian | Action: Trigger safety alert!")
        elif detected_object == "Static Object":
            print("🛑 Detected: Static Object | Action: Consider alternative route.")

    def run_agent(self, radar_stream):
        """
        Simulate real-time RADAR data input to the agent.
        :param radar_stream: List of RADAR samples
        """
        for radar_sample in radar_stream:
            print("\nProcessing RADAR data:", radar_sample)
            detected_object = self.process_radar_data(radar_sample)
            time.sleep(1)  # Simulating real-time delay

# Load the trained CNN model
model = tf.keras.models.load_model("trained_radar_cnn_model.h5")

# Create and run the AI agent
agent = RadarAI_Agent(model)
# Sample radar stream data
radar_stream = [[10, 5, 1.2, 0.9], [20, 15, -3.4, 0.8], [30, 25, 2.1, 0.7]]
agent.run_agent(radar_stream)

# Simulated incoming RADAR data stream (4 features: x, y, doppler_velocity, reflectivity)
simulated_radar_stream = [
    [12, 15, 3.0, 0.85],  # Car
    [22, 25, -1.2, 0.75],  # Pedestrian
    [32, 35, 0.0, 0.90],  # Static Object
    [10, 12, 2.5, 0.88],  # Car
    [20, 23, -2.0, 0.70],  # Pedestrian
]

# Initialize the AI agent and run it with simulated data
ai_agent = RadarAI_Agent(model)
ai_agent.run_agent(simulated_radar_stream)

# Replace this line:
# radar_input = np.array([10, 5, 1.2, 0.9]).reshape(1, 4, 1)

# With this line to add a dummy feature for 'cluster':
radar_input = np.array([10, 5, 1.2, 0.9, 0]).reshape(1, 5, 1)  # 0 represents the dummy 'cluster' value

# Continue with the prediction
prediction = agent.model.predict(radar_input)
print("Prediction Scores:", prediction)  # Check confidence for all classes

print("Model Input Shape:", agent.model.input_shape)

prediction = agent.model.predict(radar_input)
print("Prediction Scores:", prediction)  # Check confidence for all classes

import numpy as np

print("Radar Input:", radar_input)
print("Any NaN in Input:", np.isnan(radar_input).any())

for layer in agent.model.layers:
    weights = layer.get_weights()
    for w in weights:
        if np.isnan(w).any():
            print(f"🚨 NaN detected in layer {layer.name}")

import numpy as np

# Reset model weights
for layer in agent.model.layers:
    weights = layer.get_weights()
    new_weights = [np.random.randn(*w.shape) for w in weights]  # Randomized reset
    layer.set_weights(new_weights)

print("✅ Model weights reset. Retrain the model now!")

# prompt: retrain the model  now

from sklearn.utils import resample
from imblearn.over_sampling import SMOTE
from tensorflow.keras.regularizers import l2

# ... (Your existing code) ...

# Apply SMOTE to balance dataset
smote = SMOTE(sampling_strategy='auto', random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)

# Apply log-transform to reflectivity
X_train_balanced[:, 3] = np.log1p(X_train_balanced[:, 3])
X_test_scaled[:, 3] = np.log1p(X_test_scaled[:, 3])

# Reshape input for CNN
X_train_cnn = X_train_balanced.reshape((X_train_balanced.shape[0], X_train_balanced.shape[1], 1))
X_test_cnn = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))


# --- Train and Save CNN Model ---
# 1. Define Model Architecture
model = Sequential([
    Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train_cnn.shape[1], 1), padding='same'),
    BatchNormalization(),
    Conv1D(128, kernel_size=5, activation='relu', kernel_regularizer=l2(0.01), padding='same'),
    Dropout(0.3),
    Conv1D(256, kernel_size=5, activation='relu', padding='same'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')  # Output layer for 3 classes
])

# 2. Compile the Model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 3. Train the Model
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
model.fit(X_train_cnn, y_train_balanced, epochs=50, batch_size=32,
          validation_data=(X_test_cnn, y_test), callbacks=[early_stopping])

# 4. Save the Model
model.save("trained_radar_cnn_model.h5")
print("✅ Model trained and saved as 'trained_radar_cnn_model.h5'")
# --- End of Training and Saving ---

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Dense, Flatten, BatchNormalization, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# ✅ **Load and Fix RADAR Dataset**
# Simulating synthetic RADAR dataset with corrected values
np.random.seed(42)
num_samples = 200

radar_data = pd.DataFrame({
    "x": np.random.uniform(0, 50, num_samples),
    "y": np.random.uniform(0, 50, num_samples),
    "doppler_velocity": np.random.uniform(-5, 5, num_samples),
    "reflectivity": np.random.uniform(0.5, 1.0, num_samples),  # Ensuring valid values
    "class": np.random.choice(["Car", "Pedestrian", "Static Object"], num_samples)
})

# ✅ **Encode Labels**
class_mapping = {"Car": 0, "Pedestrian": 1, "Static Object": 2}
radar_data["class"] = radar_data["class"].map(class_mapping)

# ✅ **Ensure No NaN Values**
print("Any NaN in dataset?", radar_data.isna().sum())

# ✅ **Normalize Data**
scaler = MinMaxScaler()
X = scaler.fit_transform(radar_data.drop(columns=["class"]))
y = radar_data["class"].values

# ✅ **Train-Test Split**
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ✅ **Check Shapes**
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

# ✅ **Build CNN Model**
model = Sequential([
    Conv1D(32, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    Conv1D(64, kernel_size=2, activation='relu'),
    BatchNormalization(),
    Flatten(),
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(3, activation='softmax')  # 3 Classes
])

# ✅ **Compile Model**
optimizer = Adam(learning_rate=0.0001)  # Reduced LR for stability
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# ✅ **Train Model**
X_train_cnn = X_train.reshape(-1, X_train.shape[1], 1)
X_test_cnn = X_test.reshape(-1, X_test.shape[1], 1)

history = model.fit(X_train_cnn, y_train, epochs=50, batch_size=8, validation_data=(X_test_cnn, y_test))

# ✅ **Evaluate Model**
test_loss, test_acc = model.evaluate(X_test_cnn, y_test)
print(f"Final CNN Model Accuracy: {test_acc:.2f}")

# ✅ **Confusion Matrix**
y_pred = np.argmax(model.predict(X_test_cnn), axis=1)
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, cmap="coolwarm", fmt="d", xticklabels=class_mapping.keys(), yticklabels=class_mapping.keys())
plt.title("Confusion Matrix - Final Optimized CNN Model")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ✅ **Classification Report**
print(classification_report(y_test, y_pred, target_names=class_mapping.keys()))

# ✅ **Save Model**
model.save("trained_radar_cnn_model.h5")

# ✅ **Simulated AI Agent for Real-time Radar Decisions**
class RadarAI_Agent:
    def __init__(self, model):
        self.model = model

    def run_agent(self, radar_stream):
        for radar_input in radar_stream:
            # Select only the features the model was trained on (x, y, doppler_velocity, reflectivity)
            radar_input = np.array(radar_input[:4]).reshape(1, 4, 1)  # Reshape to (1, 4, 1)
            prediction = self.model.predict(radar_input)
            predicted_class = np.argmax(prediction)

            class_labels = {0: "Car", 1: "Pedestrian", 2: "Static Object"}
            action = "Continue monitoring speed & position" if predicted_class == 0 else "Caution: Possible Pedestrian!"

            print(f"🚗 Detected: {class_labels[predicted_class]} | Action: {action}")

# ✅ **Run AI Agent on Simulated Data Stream**
simulated_radar_stream = [
    [10, 15, 3.0, 0.85, 0],   # Car
    [22, 25, -1.2, 0.75, 1],  # Pedestrian
    [32, 35, 0.0, 0.90, 2],   # Static Object
    [10, 12, 2.5, 0.88, 0],   # Car
    [20, 23, -2.0, 0.70, 1]   # Pedestrian
]

ai_agent = RadarAI_Agent(model)
ai_agent.run_agent(simulated_radar_stream)

# Fixing Class Imbalance with Oversampling
from imblearn.over_sampling import SMOTE

smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

# Train-test split again with balanced data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Define an improved CNN model
model = Sequential([
    Conv1D(64, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    Conv1D(128, kernel_size=2, activation='relu'),
    BatchNormalization(),
    Flatten(),
    Dense(64, activation='relu'),
    Dropout(0.3),  # Increased dropout for regularization
    Dense(3, activation='softmax')
])

# Compile with an optimized learning rate
model.compile(optimizer=Adam(learning_rate=0.0005), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Visualizing Training Loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

from tensorflow.keras.layers import Dropout

model.add(Dropout(0.3))  # Add Dropout after dense layers

from tensorflow.keras.optimizers import Adam

optimizer = Adam(learning_rate=0.0005)  # Reduce learning rate
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

# Load labeled RADAR dataset
radar_data = pd.read_csv("/content/data/merged_radar_data.csv")

# Split Data
X = radar_data.drop("class", axis=1)
y = radar_data["class"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert Labels to Categorical
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

!ls -l

# Load labeled RADAR dataset
# radar_data = pd.read_csv("/content/data/merged_radar_data.csv") # Original problematic line
radar_data = pd.read_csv("merged_radar_data.csv") # Changed to read from current directory

# Split Data
X = radar_data.drop("label", axis=1)  # Assuming 'label' is the target column instead of 'class'
y = radar_data["label"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert Labels to Categorical
y_train_cat = to_categorical(y_train, num_classes=3) # Assuming 3 classes
y_test_cat = to_categorical(y_test, num_classes=3) # Assuming 3 classes

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# 1. Check if the file exists in the current directory
file_path = "merged_radar_data.csv"
if not os.path.exists(file_path):
    # If not, provide more details or instructions
    raise FileNotFoundError(f"The file '{file_path}' was not found in the current directory. Please make sure it exists or provide the correct path.")

# 2. Load the labeled RADAR dataset
radar_data = pd.read_csv(file_path)

# 3. Split Data (assuming 'label' is the target column)
X = radar_data.drop("label", axis=1)
y = radar_data["label"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Normalize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Convert Labels to Categorical (assuming 3 classes)
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# 1. Check if the file exists in the current directory
file_path = "merged_radar_data.csv"
# If you saved the file to a different location, replace "merged_radar_data.csv"
# with the actual file path
# For example:
# file_path = "/content/data/merged_radar_data.csv"

if not os.path.exists(file_path):
    # If not, provide more details or instructions
    raise FileNotFoundError(f"The file '{file_path}' was not found. Please make sure it exists or provide the correct path.")

# 2. Load the labeled RADAR dataset
radar_data = pd.read_csv(file_path)

# 3. Split Data (assuming 'label' is the target column)
X = radar_data.drop("label", axis=1)
y = radar_data["label"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Normalize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Convert Labels to Categorical (assuming 3 classes)
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

import os
print(os.listdir())  # List files in the current working directory

import os
print(os.listdir("/content"))  # Check files in the main directory

import os
import pandas as pd

# Sample real radar data
real_radar_data = pd.DataFrame({
    "timestamp": [0.1, 0.2, 0.3, 0.4, 0.5],
    "x": [10, 20, 30, 40, 50],
    "y": [5, 15, 25, 35, 45],
    "doppler_velocity": [1.2, -3.4, 2.1, -1.5, 0.3],
    "reflectivity": [0.9, 0.8, 0.7, 0.85, 0.95],
    "label": ["Car", "Pedestrian", "Static Object", "Car", "Pedestrian"]
})

# Sample synthetic radar data
synthetic_radar_data = pd.DataFrame({
    "timestamp": [0.15, 0.25, 0.35, 0.45, 0.55],
    "x": [12, 22, 32, 42, 52],
    "y": [7, 17, 27, 37, 47],
    "doppler_velocity": [1.0, -3.2, 2.3, -1.2, 0.5],
    "reflectivity": [0.88, 0.82, 0.72, 0.87, 0.92],
    "label": ["Car", "Pedestrian", "Static Object", "Car", "Pedestrian"]
})

# Merge real and synthetic datasets
merged_radar_data = pd.concat([real_radar_data, synthetic_radar_data], ignore_index=True)

merged_file_name = "merged_radar_data.csv"
merged_radar_data.to_csv(merged_file_name, index=False)
print(f"Merged dataset saved as '{merged_file_name}'")

print(os.listdir())  # It should list 'merged_radar_data.csv'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical
from sklearn.metrics import confusion_matrix, classification_report

# Load the merged dataset
radar_data = pd.read_csv("merged_radar_data.csv")

# Split features and labels
X = radar_data.drop("class", axis=1)
y = radar_data["class"]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert labels to categorical format for CNN training
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

# Check column names in the dataset
print("Columns in dataset:", radar_data.columns)

# Corrected feature and label extraction
X = radar_data.drop("label", axis=1)  # Use "label" instead of "class"
y = radar_data["label"]

# Load the merged dataset
radar_data = pd.read_csv("merged_radar_data.csv")

# Split features and labels
X = radar_data.drop("class", axis=1)
y = radar_data["class"]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert labels to categorical format for CNN training
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

# Corrected feature and label extraction
X = radar_data.drop("label", axis=1)  # Use "label" instead of "class"
y = radar_data["label"]

# Load the merged dataset
radar_data = pd.read_csv("merged_radar_data.csv")

# Split features and labels
X = radar_data.drop("class", axis=1)
y = radar_data["class"]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert labels to categorical format for CNN training
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

# Load the merged dataset
radar_data = pd.read_csv("merged_radar_data.csv")

# Split features and labels
# Use "label" instead of "class" to match the actual column name
X = radar_data.drop("label", axis=1)
y = radar_data["label"]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert labels to categorical format for CNN training
y_train_cat = to_categorical(y_train, num_classes=3)
y_test_cat = to_categorical(y_test, num_classes=3)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical

# Load the merged dataset
radar_data = pd.read_csv("merged_radar_data.csv")

# Split features and labels
# Use "label" instead of "class" to match the actual column name
X = radar_data.drop("label", axis=1)
y = radar_data["label"]

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Encode labels to numerical values
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)

# Convert labels to categorical format for CNN training
y_train_cat = to_categorical(y_train_encoded, num_classes=3)
y_test_cat = to_categorical(y_test_encoded, num_classes=3)

def add_noise(data, noise_level=0.02):
    noise = np.random.normal(0, noise_level, data.shape)
    return data + noise

X_train_aug = add_noise(X_train_scaled)
X_test_aug = add_noise(X_test_scaled)

# Define the CNN model
model = Sequential([
    Conv1D(16, kernel_size=2, activation="relu", input_shape=(4, 1)),
    Dropout(0.3),  # Dropout layer to reduce overfitting
    Conv1D(32, kernel_size=2, activation="relu"),
    Flatten(),
    Dense(64, activation="relu"),
    Dropout(0.3),
    Dense(3, activation="softmax")  # 3 classes (Car, Pedestrian, Static Object)
])

# Compile the model with a reduced learning rate
optimizer = Adam(learning_rate=0.0005)
model.compile(optimizer=optimizer, loss="categorical_crossentropy", metrics=["accuracy"])

# Set Early Stopping to stop training when val_loss stops improving
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model with augmented data
history = model.fit(X_train_aug.reshape(-1, 4, 1), y_train_cat,
                    epochs=50, batch_size=16,
                    validation_data=(X_test_aug.reshape(-1, 4, 1), y_test_cat),
                    callbacks=[early_stopping])

# Save the trained model
model.save("optimized_radar_ai_model.h5")
print("✅ Optimized Model Trained & Saved!")

# Train the model with augmented data
history = model.fit(X_train_aug[:, :4].reshape(-1, 4, 1), y_train_cat,  # Select first 4 features
                    epochs=50, batch_size=16,
                    validation_data=(X_test_aug[:, :4].reshape(-1, 4, 1), y_test_cat),  # Select first 4 features
                    callbacks=[early_stopping])

# Save the trained model
model.save("optimized_radar_ai_model.h5")
print("✅ Optimized Model Trained & Saved!")

from tensorflow import keras

# Load the model from the HDF5 file
model = keras.models.load_model("optimized_radar_ai_model.h5")

# Save the model in Keras format
model.save("optimized_radar_ai_model.keras")

print("✅ Model converted and saved in Keras format!")

from tensorflow.keras.optimizers import Adam

# Compile the model with an optimizer, loss function, and metrics
model.compile(optimizer=Adam(learning_rate=0.0005),
              loss="categorical_crossentropy",
              metrics=["accuracy"])

# Now you can train the model
history = model.fit(X_train_aug.reshape(-1, 4, 1), y_train_cat,
                    epochs=50, batch_size=16,
                    validation_data=(X_test_aug.reshape(-1, 4, 1), y_test_cat),
                    callbacks=[early_stopping])

# Train the model with augmented data
history = model.fit(X_train_aug[:, :4].reshape(-1, 4, 1), y_train_cat,  # Select first 4 features
                    epochs=50, batch_size=16,
                    validation_data=(X_test_aug[:, :4].reshape(-1, 4, 1), y_test_cat),  # Select first 4 features
                    callbacks=[early_stopping])

# Save the trained model
model.save("optimized_radar_ai_model.h5")
print("✅ Optimized Model Trained & Saved!")

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test_aug.reshape(-1, 4, 1), y_test_cat)
print(f"📊 Final Optimized Model Accuracy: {test_acc:.2f}")

# Generate predictions
y_pred = model.predict(X_test_aug.reshape(-1, 4, 1))
y_pred_classes = np.argmax(y_pred, axis=1)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(cm, annot=True, cmap="coolwarm", fmt="d", xticklabels=["Car", "Pedestrian", "Static Object"], yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Optimized Radar AI Model")
plt.show()

# Print Classification Report
print(classification_report(y_test, y_pred_classes))

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test_aug[:, :4].reshape(-1, 4, 1), y_test_cat)  # Select first 4 features for evaluation
print(f"📊 Final Optimized Model Accuracy: {test_acc:.2f}")

# Generate predictions
y_pred = model.predict(X_test_aug[:, :4].reshape(-1, 4, 1))  # Select first 4 features for prediction
y_pred_classes = np.argmax(y_pred, axis=1)

# Assuming y_test contains the original class labels (not one-hot encoded)
# If y_test is one-hot encoded, you'll need to convert it back to class labels first
# For example: y_test_classes = np.argmax(y_test, axis=1)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_classes)
sns.heatmap(cm, annot=True, cmap="coolwarm", fmt="d",
            xticklabels=["Car", "Pedestrian", "Static Object"],
            yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Optimized Radar AI Model")
plt.show()

# Print Classification Report
print(classification_report(y_test, y_pred_classes))

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, classification_report

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(X_test_aug[:, :4].reshape(-1, 4, 1), y_test_cat)  # Select first 4 features for evaluation
print(f"📊 Final Optimized Model Accuracy: {test_acc:.2f}")

# Generate predictions
y_pred = model.predict(X_test_aug[:, :4].reshape(-1, 4, 1))  # Select first 4 features for prediction
y_pred_classes = np.argmax(y_pred, axis=1)

# Map predicted numerical labels back to original string labels
class_labels = {0: "Car", 1: "Pedestrian", 2: "Static Object"}
y_pred_labels = [class_labels[i] for i in y_pred_classes] # Convert numerical labels to string labels

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_labels)  # Use string labels for both y_true and y_pred
sns.heatmap(cm, annot=True, cmap="coolwarm", fmt="d",
            xticklabels=["Car", "Pedestrian", "Static Object"],
            yticklabels=["Car", "Pedestrian", "Static Object"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - Optimized Radar AI Model")
plt.show()

# Print Classification Report
print(classification_report(y_test, y_pred_labels)) # Use string labels for both y_true and y_pred

# Plot Training vs Validation Loss
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.title("Training vs Validation Loss - Optimized Model")
plt.show()