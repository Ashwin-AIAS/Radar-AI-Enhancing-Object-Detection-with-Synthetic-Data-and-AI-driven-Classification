import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Input
from tensorflow.keras.optimizers import Adam

# Define input shape
input_dim = 100

# Generator Model
def build_generator():
    model = Sequential([
        Dense(128, input_dim=input_dim),
        LeakyReLU(alpha=0.2),
        BatchNormalization(),
        Dense(256),
        LeakyReLU(alpha=0.2),
        BatchNormalization(),
        Dense(512),
        LeakyReLU(alpha=0.2),
        BatchNormalization(),
        Dense(4, activation='tanh')  # Output: X, Y, Doppler velocity, Reflectivity
    ])
    return model

# Discriminator Model
def build_discriminator():
    model = Sequential([
        Dense(512, input_dim=4),
        LeakyReLU(alpha=0.2),
        Dense(256),
        LeakyReLU(alpha=0.2),
        Dense(128),
        LeakyReLU(alpha=0.2),
        Dense(1, activation='sigmoid')
    ])
    return model

# Compile models
def compile_models():
    discriminator = build_discriminator()
    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])
    
    generator = build_generator()
    
    z = Input(shape=(input_dim,))
    fake_data = generator(z)
    discriminator.trainable = False
    validity = discriminator(fake_data)
    
    combined = Model(z, validity)
    combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
    
    return generator, discriminator, combined

# Training GAN
def train_gan(epochs=10000, batch_size=32):
    generator, discriminator, combined = compile_models()
    
    real_data = np.random.normal(0, 1, (1000, 4))  # Simulated real RADAR data
    
    for epoch in range(epochs):
        idx = np.random.randint(0, real_data.shape[0], batch_size)
        real_samples = real_data[idx]
        noise = np.random.normal(0, 1, (batch_size, input_dim))
        fake_samples = generator.predict(noise)
        
        d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))
        d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((batch_size, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)
        
        noise = np.random.normal(0, 1, (batch_size, input_dim))
        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))
        
        if epoch % 1000 == 0:
            print(f"Epoch {epoch}: D Loss={d_loss[0]}, G Loss={g_loss}")
    
    generator.save("generator_model.h5")
    discriminator.save("discriminator_model.h5")
    print("GAN Training Complete and Models Saved.")

if __name__ == "__main__":
    train_gan()
